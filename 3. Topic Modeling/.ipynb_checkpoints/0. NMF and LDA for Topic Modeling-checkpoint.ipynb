{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "#from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    d = defaultdict(list)\n",
    "\n",
    "    parent_dir = '../Data Sets/'\n",
    "\n",
    "    path_dic = {'B': 'business_s.csv', 'R':'review_m.csv'}\n",
    "#                ,'C':'checkin.csv', 'T':'tip.csv', 'U':'user.csv'}\n",
    "\n",
    "    for key in path_dic:\n",
    "        d[key] = pd.read_csv(parent_dir + path_dic[key]).drop('Unnamed: 0', 1)\n",
    "    return d\n",
    "def show():\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "    \n",
    "def clean_format(w):\n",
    "    w = w.lower().replace('.', '').replace(',', '').replace('!', '')\n",
    "    #.replace('+', '').replace('(', '').replace(')', '')\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.random.seed(47)\n",
    "# parent_dir = '../Data Sets/'\n",
    "# temp = d['R'].sample(frac = 0.1, replace = False)\n",
    "# temp.shape\n",
    "# temp.to_csv(parent_dir + 'review_m.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zihaoxu/anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2821: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  if self.run_code(code, result):\n"
     ]
    }
   ],
   "source": [
    "d = read_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437631, 25)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['RB'] = d['R'].merge(d['B'], on = 'business_id', how = 'inner')\n",
    "d['RB'] = d['RB'].dropna(subset = ['is_open'])\n",
    "d['RB'].rename(columns = {'stars_x' : 'review_star', 'stars_y':'buz_star'}, inplace = True)\n",
    "d['RB'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business_id', 'cool', 'date', 'funny', 'review_id', 'review_star', 'text', 'useful', 'user_id', 'address', 'attributes', 'categories', 'city', 'hours', 'is_open', 'latitude', 'longitude', 'name', 'neighborhood', 'postal_code', 'review_count', 'buz_star', 'state', 'price', 'credit_card']\n"
     ]
    }
   ],
   "source": [
    "print(list(d['RB']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(437631, 25)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = d['RB']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-112-d5b2077376ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-d5b2077376ec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-d5b2077376ec>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/zihaoxu/anaconda/lib/python3.5/site-packages/nltk/stem/porter.py\u001b[0m in \u001b[0;36mstem\u001b[0;34m(self, word)\u001b[0m\n\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0mstem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_replace_suffix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'e'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 632\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    633\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mstem\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_measure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ends_cvc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Stem all the words in reviews\n",
    "ps = PorterStemmer()\n",
    "\n",
    "df.index = range(len(df))\n",
    "df['text'] = [' '.join([ps.stem(w) for w in df['text'][i].split()]) for i in range(len(df['text']))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'], df['state'], train_size = .8, random_state = 47)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF and LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deafult values and helper function to print the topics\n",
    "n_features = 5000\n",
    "n_components = 10\n",
    "n_top = 15\n",
    "\n",
    "def print_top_words(model, feature_names, n_top):\n",
    "    for topic_ind, topic in enumerate(model.components_):\n",
    "        message = \"Topic %d: \" % topic_ind\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF extracted in 109.040s\n",
      "TFIDF extracted in 130.580s\n"
     ]
    }
   ],
   "source": [
    "# Extract TF for LDA:\n",
    "t0 = time()\n",
    "count_vect = CountVectorizer(max_df = .95, min_df = 3, ngram_range = (1,2), \\\n",
    "                             max_features = n_features, stop_words = 'english')\n",
    "tf = count_vect.fit_transform(X_train)\n",
    "print(\"TF extracted in %0.3fs\" % (time() - t0))\n",
    "\n",
    "# Extract TF-IDF for NMF:\n",
    "t0 = time()\n",
    "tfidf_vect = TfidfVectorizer(max_df = .95, min_df = 3, ngram_range = (1,2), \\\n",
    "                             max_features = n_features, stop_words = 'english')\n",
    "tfidf = tfidf_vect.fit_transform(X_train)\n",
    "print(\"TFIDF extracted in %0.3fs\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting the NMF & LDA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 157.999\n",
      "\n",
      "Topics from NMF model: \n",
      "Topic 0: good pretty food good pretty good good food good service service good really good service good place place prices price place good overall\n",
      "Topic 1: like just don car know store work people new make time day going need way\n",
      "Topic 2: great great service great food food great service great great place service atmosphere place great great atmosphere great experience awesome great job prices time\n",
      "Topic 3: pizza crust cheese wings pizzas toppings best pizza pepperoni sauce slice pizza place delivery good pizza pizza good great pizza\n",
      "Topic 4: ice cream ice cream chocolate coffee tea flavors dessert cake cookies vanilla cone cookie sweet waffle\n",
      "Topic 5: service customer customer service great customer excellent horrible service great excellent customer rude great service worst excellent service manager poor thank\n",
      "Topic 6: burger fries burgers cheese bun bacon onion rings onion rings burger fries shake sweet potato patty potato potato fries\n",
      "Topic 7: delicious ordered menu salad cheese restaurant sauce try sandwich fresh meal lunch bread meat steak\n",
      "Topic 8: sushi rolls roll fish fresh sashimi ayce sushi place tuna eat salmon quality rice best sushi japanese\n",
      "Topic 9: recommend highly highly recommend recommend place professional amazing definitely recommend dr work definitely recommended highly recommended excellent job company\n",
      "Topic 10: minutes order time came wait got said took asked told table didn went 10 waited\n",
      "Topic 11: love place love place love love great place amazing favorite awesome place great absolutely love coffee recommend place absolutely try come\n",
      "Topic 12: chicken rice fried thai curry fried rice fried chicken spicy sauce pad soup beef pad thai ordered wings\n",
      "Topic 13: food restaurant service mexican great food food good quality food service fast food great good food eat mexican food chinese better\n",
      "Topic 14: staff friendly staff friendly friendly staff helpful clean super super friendly nice friendly helpful dr location staff super friendly service coffee\n",
      "Topic 15: best ve vegas amazing las las vegas town far time best ve years times hands far best favorite\n",
      "Topic 16: room hotel stay rooms vegas pool strip nice stayed casino clean night bed desk floor\n",
      "Topic 17: hair salon cut job did color stylist nails hair cut haircut nail great job appointment amazing wanted\n",
      "Topic 18: bar beer happy drinks happy hour hour night drink selection fun music bartender beers wine patio\n",
      "Topic 19: really really good nice really nice really enjoyed like really like food really enjoyed really great really really place really liked place really liked\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "t0 = time()\n",
    "nmf = NMF(n_components = 20, random_state = 47, alpha = .1, l1_ratio = .5).fit(tfidf)\n",
    "print(\"done in %0.3f\" % (time() - t0))\n",
    "print(\"\\nTopics from NMF model: \")\n",
    "tfidf_feature_names = tfidf_vect.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 787.078s.\n",
      "Topic 0: place great love staff friendly amazing definitely awesome favorite great place recommend love place best sandwiches absolutely\n",
      "Topic 1: cream ice tea chocolate ice cream cake sweet try delicious flavors like milk love dessert taste\n",
      "Topic 2: like don just place know really want think good people easy way thing price better\n",
      "Topic 3: chicken sauce ordered rice fried like good beef pork food soup spicy flavor meat salad\n",
      "Topic 4: feel hair did job taking like appointment feel like amazing cut salon nails massage happy clean\n",
      "Topic 5: ve time years make staff sure times care going dr family year make sure ago office\n",
      "Topic 6: bar place nice area selection good staff location drinks friendly drink like great local little\n",
      "Topic 7: best vegas ve las las vegas italian bartender far strip amazing town favorite eaten cocktails orange\n",
      "Topic 8: food great service pizza good atmosphere friendly amazing great service delicious great food service great nice food great place\n",
      "Topic 9: said minutes time service asked told didn got just customer order went came did took\n",
      "Topic 10: really great time got nice super didn did experience wanted new try went definitely little\n",
      "Topic 11: recommend happy excellent highly beer highly recommend hour great service happy hour beers team selection awesome definitely\n",
      "Topic 12: good really food place lunch fresh fish ve pretty really good menu fast eat wings try\n",
      "Topic 13: coffee breakfast eggs brunch toast morning cafe cup saturday pancakes bacon french hidden road egg\n",
      "Topic 14: dinner restaurant salad menu meal wine steak ordered cheese server delicious table bread dessert dining\n",
      "Topic 15: burger fries sandwich cheese dog burgers bread bacon good miss dogs got hot just meat\n",
      "Topic 16: night time great fun pool kids music club old son play wife really went money\n",
      "Topic 17: food good service sushi restaurant table wait place waitress pretty ordered came order time server\n",
      "Topic 18: room hotel stay parking clean nice free line strip rooms casino area park check stayed\n",
      "Topic 19: car store service company work new home price called shop day use business buy house\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the LDA model\n",
    "t0 = time()\n",
    "lda = LatentDirichletAllocation(n_topics = 20, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=47).fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print_top_words(lda, tfidf_feature_names, n_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lda.__dict__\n",
    "# import inspect\n",
    "# inspect.getmembers(lda, predicate=inspect.ismethod)\n",
    "\n",
    "\n",
    "save_model = open(\"pickled_algos/lda.pickle\",\"wb\")\n",
    "pickle.dump(lda, save_model)\n",
    "save_model.close()\n",
    "\n",
    "save_model = open(\"pickled_algos/nmf.pickle\",\"wb\")\n",
    "pickle.dump(nmf, save_model)\n",
    "save_model.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-178950.47513472088"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda.score(tfidf_vect.fit_transform(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
