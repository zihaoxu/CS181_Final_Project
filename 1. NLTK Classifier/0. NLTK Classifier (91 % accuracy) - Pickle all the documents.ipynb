{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "#from nltk.corpus import movie_reviews\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "import pickle\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "from nltk.classify import ClassifierI\n",
    "from statistics import mode\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_files():\n",
    "    d = defaultdict(list)\n",
    "\n",
    "    parent_dir = '../Data Sets/'\n",
    "\n",
    "    path_dic = {'B': 'business_s.csv', 'C':'checkin.csv', 'R':'review_s.csv'\\\n",
    "               , 'T':'tip.csv', 'U':'user.csv'}\n",
    "\n",
    "    for key in path_dic:\n",
    "        d[key] = pd.read_csv(parent_dir + path_dic[key]).drop('Unnamed: 0', 1)\n",
    "    return d\n",
    "def show():\n",
    "    sns.despine()\n",
    "    plt.show()\n",
    "    \n",
    "def clean_format(w):\n",
    "    w = w.lower().replace('.', '').replace(',', '').replace('!', '')\n",
    "    #.replace('+', '').replace('(', '').replace(')', '')\n",
    "    return w\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d = read_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK: creating classifiers, pickle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "d['RB'] = d['R'].merge(d['B'], on = 'business_id', how = 'inner')\n",
    "d['RB'] = d['RB'].dropna(subset = ['is_open'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>business_id</th>\n",
       "      <th>cool</th>\n",
       "      <th>date</th>\n",
       "      <th>funny</th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_star</th>\n",
       "      <th>text</th>\n",
       "      <th>useful</th>\n",
       "      <th>user_id</th>\n",
       "      <th>address</th>\n",
       "      <th>...</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>neighborhood</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>review_count</th>\n",
       "      <th>buz_star</th>\n",
       "      <th>state</th>\n",
       "      <th>price</th>\n",
       "      <th>credit_card</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fjMXGgOr3aCxnN48kovZ_Q</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-03-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3BBCHVND9tDPNliTFoLCHA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>We recently decided to give this place another...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>bCrpStRCku_gEX3Iwuv94A</td>\n",
       "      <td>5051 W Craig Rd</td>\n",
       "      <td>...</td>\n",
       "      <td>36.238959</td>\n",
       "      <td>-115.211568</td>\n",
       "      <td>Craig Road Animal Hospital</td>\n",
       "      <td>Northwest</td>\n",
       "      <td>89130</td>\n",
       "      <td>192</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NV</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              business_id  cool        date  funny               review_id  \\\n",
       "0  fjMXGgOr3aCxnN48kovZ_Q     0  2015-03-09    0.0  3BBCHVND9tDPNliTFoLCHA   \n",
       "\n",
       "   review_star                                               text  useful  \\\n",
       "0          5.0  We recently decided to give this place another...     0.0   \n",
       "\n",
       "                  user_id          address     ...       latitude   longitude  \\\n",
       "0  bCrpStRCku_gEX3Iwuv94A  5051 W Craig Rd     ...      36.238959 -115.211568   \n",
       "\n",
       "                         name neighborhood  postal_code  review_count  \\\n",
       "0  Craig Road Animal Hospital    Northwest        89130           192   \n",
       "\n",
       "   buz_star state price credit_card  \n",
       "0       4.0    NV   NaN         NaN  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d['RB'].rename(columns = {'stars_x' : 'review_star', 'stars_y':'buz_star'}, inplace = True)\n",
    "d['RB'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business_id', 'cool', 'date', 'funny', 'review_id', 'review_star', 'text', 'useful', 'user_id', 'address', 'attributes', 'categories', 'city', 'hours', 'is_open', 'latitude', 'longitude', 'name', 'neighborhood', 'postal_code', 'review_count', 'buz_star', 'state', 'price', 'credit_card']\n"
     ]
    }
   ],
   "source": [
    "print(list(d['RB']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "documents = [(t, star) for t,star in zip(d['RB']['text'], d['RB']['review_star'])]\n",
    "\n",
    "save_documents = open(\"pickled_algos/documents.pickle\",\"wb\")\n",
    "pickle.dump(documents, save_documents)\n",
    "save_documents.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take look at the first review:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"We recently decided to give this place another try after having issues at our normal vet and I'm very glad we did! We transferred all of our pets records over here and plan on using them and only them. The receptionists are always on top of everything, friendly and helpful. Both the vet techs as well as the vets have also been very friendly, helpful and knowledgeable. We had an incident with one of or dogs, Miracle, and Dr.Finder took GREAT care of her and now Miracle LOVES her. We are very pleased with the treatment our animals have been receiving here!\", 5.0)\n"
     ]
    }
   ],
   "source": [
    "print(documents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['python', 'python', 'python', 'python', 'pythonli']\n"
     ]
    }
   ],
   "source": [
    "# We will use the stemmer to stem all the words\n",
    "ps = PorterStemmer()\n",
    "example_words = [\"python\",\"pythoner\",\"pythoning\",\"pythoned\",\"pythonly\"]\n",
    "\n",
    "print([ps.stem(w) for w in example_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove stop words? Something to consider. But turned out this decreases accuracy..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This nonsense. I hate place. The food bad service terrible\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    " \n",
    "data = \"This is nonsense. I hate this place. The food is bad and the service is terrible\"\n",
    "stopWords = list(set(stopwords.words('english')))\n",
    "\n",
    "# stopWords[:5]\n",
    "print(' '.join([w for w in data.split() if w not in stopWords ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('the', 25207), ('and', 17402), ('i', 13434), ('a', 12866), ('to', 12566), ('wa', 9003), ('of', 7497), ('it', 6576), ('for', 6096), ('is', 5996), ('in', 5677), ('my', 4878), ('that', 4374), ('with', 4131), ('we', 4044)]\n",
      "stupid appeared: 26 times\n"
     ]
    }
   ],
   "source": [
    "all_words = []\n",
    "\n",
    "for (t, star) in documents:\n",
    "    for word in t.split():\n",
    "        w = clean_format(word)\n",
    "        all_words.append(ps.stem(w))\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)\n",
    "print(all_words.most_common(15))\n",
    "print(\"stupid appeared: \" + str(all_words['stupid']) + \" times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'and', 'i', 'a', 'to', 'wa', 'of', 'it', 'for', 'is', 'in', 'my', 'that', 'with', 'we', 'thi', 'they', 'but', 'you', 'on', 'have', 'had', 'not', 'so', 'were', 'be', 'at', 'place', 'are', 'food', 'good', 'as', 'time', 'me', 'great', 'veri', 'there', 'like', 'get', 'all', 'go', 'out', 'if', 'our', 'just', 'here', 'servic', 'one', 'from', 'order', 'when', 'would', 'their', 'back', 'up', 'or', 'an', 'he', \"it'\", 'will', 'about', 'tri', 'realli', 'she', 'your', 'which', 'what', 'been', 'some', 'come', 'love', 'no', 'us', 'also', 'even', 'becaus', 'more', 'onli', 'other', 'can', 'do', '-', 'got', 'by', 'nice', 'look', 'want', 'them', 'make', \"don't\", 'wait', 'price', 'best', 'after', 'ha', \"i'm\", 'too', 'well', \"i'v\", \"didn't\", 'came', 'restaur', 'than', 'went', 'ask', 'did', 'day', 'friendli', 'work', 'over', 'first', 'littl', 'never', 'alway', 'staff', 'drink', 'her', 'could', 'menu', 'definit', 'peopl', 'know', 'take', 'recommend', 'say', 'thing', 'experi', 'need', 'tast', 'much', 'chicken', 'then', 'pretti', 'made', 'how', 'who', 'way', 'befor', 'again', 'friend', 'tabl', 'call', 'delici', 'amaz', 'see', 'night', 'eat', 'think', 'said', 'custom', 'room', 'better', 'minut', 'sure', '&', 'lot', 'bar', 'two', 'give', 'help', 'off', 'ever', 'locat', 'ani', 'around', 'sinc', 'use', 'still', 'pizza', 'dish', 'new', 'down', 'few', 'sauc', 'star', 'busi', 'hour', 'now', 'walk', 'vega', '', 'while', 'took', 'right', 'check', 'am', 'review', 'hi', 'everyth', 'next', 'fresh', 'feel', 'year', 'find', 'visit', \"wasn't\", 'fri', 'clean', 'everi', 'enjoy']\n"
     ]
    }
   ],
   "source": [
    "words_features = [s for (s,_) in list(all_words.most_common(5000))]\n",
    "\n",
    "save_word_features = open(\"pickled_algos/word_features5k.pickle\",\"wb\")\n",
    "pickle.dump(words_features, save_word_features)\n",
    "save_word_features.close()\n",
    "\n",
    "print(words_features[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'top': True, 'use': True, 'both': True, 'an': True, 'care': True, 'of': True, 'help': True, 'now': True, 'her': True, 'them': True, 'give': True, 'are': True, 'treatment': True, 'at': True, 'also': True, 'or': True, 'after': True, 'vet': True, 'to': True, 'pet': True, \"i'm\": True, 'veri': True, 'and': True, 'we': True, 'plan': True, 'the': True, 'record': True, 'transfer': True, 'alway': True, 'drfinder': False, 'great': True, 'glad': True, 'one': True, 'with': True, 'over': True, 'normal': True, 'decid': True, 'love': True, 'knowledg': True, 'on': True, 'place': True, 'miracl': True, 'thi': True, 'everyth': True, 'anim': True, 'been': True, 'here': True, 'our': True, 'took': True, 'issu': True, 'incid': True, 'have': True, 'dog': True, 'onli': True, 'friendli': True, 'tech': True, 'tri': True, 'did': True, 'had': True, 'as': True, 'recent': True, 'receptionist': True, 'pleas': True, 'all': True, 'well': True, 'receiv': True, 'anoth': True}, 5.0)\n"
     ]
    }
   ],
   "source": [
    "def find_features(document):\n",
    "    words = set(document.split())\n",
    "    features = {}\n",
    "    for w in words:\n",
    "        w = clean_format(w)\n",
    "        w = ps.stem(w)\n",
    "        features[w] = (w in words_features)\n",
    "    return features\n",
    "\n",
    "featuresets = [(find_features(doc), star) for (doc,star) in documents]\n",
    "\n",
    "save_word_features = open(\"pickled_algos/featuresets.pickle\",\"wb\")\n",
    "pickle.dump(word_features, save_word_features)\n",
    "save_word_features.close()\n",
    "\n",
    "print(featuresets[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4349"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featuresets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos rate:  0.7429245283018868\n",
      "len of temp:  3816\n"
     ]
    }
   ],
   "source": [
    "# POS to NEG encode\n",
    "temp = []\n",
    "pos_count = 0\n",
    "for i in range(len(featuresets)):\n",
    "    if featuresets[i][1] >= 4:\n",
    "        temp.append((featuresets[i][0], 'pos'))\n",
    "        pos_count = pos_count + 1\n",
    "    elif featuresets[i][1] <= 2:\n",
    "        temp.append((featuresets[i][0], 'neg'))\n",
    "print(\"pos rate: \", pos_count / len(temp))\n",
    "print(\"len of temp: \", len(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trying out some clfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "816\n"
     ]
    }
   ],
   "source": [
    "training_set = temp[:3000]\n",
    "testing_set = temp[3000:]\n",
    "\n",
    "print(len(training_set))\n",
    "print(len(testing_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Classifier accuracy:  46.200980392156865\n",
      "Most Informative Features\n",
      "                  refund = True              neg : pos    =     49.9 : 1.0\n",
      "                    zero = True              neg : pos    =     23.2 : 1.0\n",
      "                   bewar = True              neg : pos    =     21.7 : 1.0\n",
      "                    rude = True              neg : pos    =     21.7 : 1.0\n",
      "                    dirt = True              neg : pos    =     19.8 : 1.0\n",
      "                  garbag = True              neg : pos    =     19.8 : 1.0\n",
      "                   ignor = True              neg : pos    =     18.6 : 1.0\n",
      "                 terribl = True              neg : pos    =     18.5 : 1.0\n",
      "              underwhelm = True              neg : pos    =     17.9 : 1.0\n",
      "               tasteless = True              neg : pos    =     17.9 : 1.0\n",
      "                knowledg = True              pos : neg    =     17.8 : 1.0\n",
      "                  intent = True              neg : pos    =     16.0 : 1.0\n",
      "                    shut = True              neg : pos    =     16.0 : 1.0\n",
      "              seriously? = True              neg : pos    =     16.0 : 1.0\n",
      "                   upset = True              neg : pos    =     15.7 : 1.0\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4747)\n",
    "\n",
    "clf = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Naive Bayes Classifier accuracy: \", nltk.classify.accuracy(clf, testing_set) * 100)\n",
    "clf.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combing multiple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Naive Bayes Algo accuracy percent: 46.200980392156865\n",
      "Most Informative Features\n",
      "                  refund = True              neg : pos    =     49.9 : 1.0\n",
      "                    zero = True              neg : pos    =     23.2 : 1.0\n",
      "                   bewar = True              neg : pos    =     21.7 : 1.0\n",
      "                    rude = True              neg : pos    =     21.7 : 1.0\n",
      "                    dirt = True              neg : pos    =     19.8 : 1.0\n",
      "                  garbag = True              neg : pos    =     19.8 : 1.0\n",
      "                   ignor = True              neg : pos    =     18.6 : 1.0\n",
      "                 terribl = True              neg : pos    =     18.5 : 1.0\n",
      "              underwhelm = True              neg : pos    =     17.9 : 1.0\n",
      "               tasteless = True              neg : pos    =     17.9 : 1.0\n",
      "                knowledg = True              pos : neg    =     17.8 : 1.0\n",
      "                  intent = True              neg : pos    =     16.0 : 1.0\n",
      "                    shut = True              neg : pos    =     16.0 : 1.0\n",
      "              seriously? = True              neg : pos    =     16.0 : 1.0\n",
      "                   upset = True              neg : pos    =     15.7 : 1.0\n",
      "MNB_classifier accuracy percent: 88.84803921568627\n",
      "BernoulliNB_classifier accuracy percent: 79.41176470588235\n",
      "LogisticRegression_classifier accuracy percent: 91.54411764705883\n",
      "LinearSVC_classifier accuracy percent: 89.70588235294117\n",
      "NuSVC_classifier accuracy percent: 81.00490196078431\n",
      "SGDClassifier accuracy percent: 89.58333333333334\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(4747)\n",
    "\n",
    "classifier = nltk.NaiveBayesClassifier.train(training_set)\n",
    "print(\"Original Naive Bayes Algo accuracy percent:\", (nltk.classify.accuracy(classifier, testing_set))*100)\n",
    "classifier.show_most_informative_features(15)\n",
    "\n",
    "###############\n",
    "save_classifier = open(\"pickled_algos/originalnaivebayes5k.pickle\",\"wb\")\n",
    "pickle.dump(classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "MNB_classifier = SklearnClassifier(MultinomialNB())\n",
    "MNB_classifier.train(training_set)\n",
    "print(\"MNB_classifier accuracy percent:\", (nltk.classify.accuracy(MNB_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/MNB_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(MNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "BernoulliNB_classifier = SklearnClassifier(BernoulliNB())\n",
    "BernoulliNB_classifier.train(training_set)\n",
    "print(\"BernoulliNB_classifier accuracy percent:\", (nltk.classify.accuracy(BernoulliNB_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/BernoulliNB_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(BernoulliNB_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "LogisticRegression_classifier = SklearnClassifier(LogisticRegression())\n",
    "LogisticRegression_classifier.train(training_set)\n",
    "print(\"LogisticRegression_classifier accuracy percent:\", (nltk.classify.accuracy(LogisticRegression_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/LogisticRegression_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(LogisticRegression_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "LinearSVC_classifier = SklearnClassifier(LinearSVC())\n",
    "LinearSVC_classifier.train(training_set)\n",
    "print(\"LinearSVC_classifier accuracy percent:\", (nltk.classify.accuracy(LinearSVC_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/LinearSVC_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(LinearSVC_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "NuSVC_classifier = SklearnClassifier(NuSVC())\n",
    "NuSVC_classifier.train(training_set)\n",
    "print(\"NuSVC_classifier accuracy percent:\", (nltk.classify.accuracy(NuSVC_classifier, testing_set))*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/NuSVC_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(LinearSVC_classifier, save_classifier)\n",
    "save_classifier.close()\n",
    "\n",
    "SGDC_classifier = SklearnClassifier(SGDClassifier())\n",
    "SGDC_classifier.train(training_set)\n",
    "print(\"SGDClassifier accuracy percent:\",nltk.classify.accuracy(SGDC_classifier, testing_set)*100)\n",
    "\n",
    "save_classifier = open(\"pickled_algos/SGDC_classifier5k.pickle\",\"wb\")\n",
    "pickle.dump(SGDC_classifier, save_classifier)\n",
    "save_classifier.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's combine the clfs！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "voted_classifier accuracy percent: 91.05392156862744\n"
     ]
    }
   ],
   "source": [
    "class VoteClassifier(ClassifierI):\n",
    "    def __init__(self, *classifiers):\n",
    "        self._classifiers = classifiers\n",
    "\n",
    "    def classify(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "        return mode(votes)\n",
    "\n",
    "    def confidence(self, features):\n",
    "        votes = []\n",
    "        for c in self._classifiers:\n",
    "            v = c.classify(features)\n",
    "            votes.append(v)\n",
    "\n",
    "        chosen_class = votes.count(mode(votes))\n",
    "        return chosen_class / len(votes)\n",
    "    \n",
    "voted_classifier = VoteClassifier(SGDClassifier_classifier,\n",
    "                                  LogisticRegression_classifier, \n",
    "                                  LinearSVC_classifier) \n",
    "#                                   MNB_classifier, \n",
    "#                                   NuSVC_classifier)\n",
    "print(\"voted_classifier accuracy percent:\", (nltk.classify.accuracy(voted_classifier, testing_set))*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "91.17 % is really impressive!!!!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment(text):\n",
    "    feats = find_features(text)\n",
    "    return voted_classifier.classify(feats),voted_classifier.confidence(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'you': True, 'diesel': True, 'earn': True, 'back': True, 'i': True, 'nice': True, 'a': True, 'it': True, 'are': True, 'guy': True, 'these': True, 'help': True, 'busi': True, 'compani': True, 'have': True, 'your': True, 'to': True, 'if': True, 'do': True, 'am': True, 'friendli': True, 'and': True, 'will': True, 'take': True, 'the': True, 'super': True, 'real': True, 'veri': True, 'what': True, 'they': True, 'respect': True, 'go': True, 'deal': True}, 'pos')\n",
      "('pos', 1.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "\n",
    "    print(testing_set[i])\n",
    "    print(sentiment(testing_set[i][1]))\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the misclassified ones..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = defaultdict(list)\n",
    "for i in range(len(testing_set)):\n",
    "    pred, conf = sentiment(testing_set[i][1])\n",
    "    result['pred'].append(pred)\n",
    "    result['pred'].append(pred)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
